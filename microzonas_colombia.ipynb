{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports y configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd # Point para crear geometrías de puntos\n",
    "import numpy as np\n",
    "from shapely.geometry import Point # Point para crear geometrías de puntos\n",
    "from shapely.ops import unary_union, voronoi_diagram # unary_union: Sirve para fusionar muchas geometrías en una sola\n",
    "from tqdm.auto import tqdm # Crea barras de progreso\n",
    "import osmnx as ox\n",
    "import folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "from IPython.display import display # Fuerza a Jupyter a mostrar objetos\n",
    "import warnings # Ocultar los warnings no críticos para mantener la salida limpia\n",
    "import unicodedata # obtener información detallada sobre cualquier carácter Unicode (como su nombre, clase bidireccional,etc.)\n",
    "from azure.storage.blob import BlobServiceClient # Conexión con Azure\n",
    "from pyspark.sql import SparkSession # pyspark\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Configuración de librerías\n",
    "warnings.filterwarnings(\"ignore\") # Ocultar los warnings no críticos para mantener la salida limpia\n",
    "ox.settings.log_console = False # No llenar la consola de mensajes de registro\n",
    "ox.settings.use_cache = True # OSMnx para guardar en memoria caché las descargas de mapas la segunda vez es más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "access_key = os.getenv(\"AZURE_STORAGE_KEY\")\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Crea el \"puente\" para poder entrar a los archivos guardados en Azure\n",
    "\n",
    "# Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MicrozonasColombia\") # Le pone nombre a esta tarea\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") # Reserva 4GB de memoria para resultados\n",
    "    .getOrCreate() # Enciende el motor de Spark\n",
    ") \n",
    "\n",
    "# Da la llave de Azure a Spark para que pueda leer los archivos directamente desde el \"Data Lake\" (el almacén de datos masivo)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "    \"fs.azure.account.key.kodatasciencedatalake.dfs.core.windows.net\",\n",
    "    access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRS\n",
    "EPSG_WGS84 = \"EPSG:4326\" # Es el sistema de Latitud/Longitud (el que usa el GPS y Google Maps)\n",
    "EPSG_M = \"EPSG:3857\" # Si intentas calcular áreas o distancias usando grados (4326), el cálculo saldrá mal. Para medir metros cuadrados o distancias reales, debes convertir los datos a este sistema (3857)\n",
    "\n",
    "# Rangos de clasificación de microzonas\n",
    "RANGOS = {\n",
    "    \"chica\": (0, 50),\n",
    "    \"mediana\": (50, 100),\n",
    "    \"grande\": (100, 200),\n",
    "    \"separar\": (200, 1e9)\n",
    "}\n",
    "\n",
    "# Configuración Data Lake (si para guardar en Azure)\n",
    "CONTAINER_DATALAKE = \"dl-plata\"\n",
    "DATALAKE_NAME = \"kodatasciencedatalake\"\n",
    "PROYECTO = \"microzonas\"\n",
    "CATEGORIA = \"microzonas_colombia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Recopilación y filtrado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Limpieza y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpieza  de texto: elimina tildes, espacios extras, caracteres especiales.\"\"\"\n",
    "    if pd.isna(texto): # protege la función si le llega un valor vacío\n",
    "        return \"\"\n",
    "\n",
    "    # Convierte el dato a texto (por si llega un número) y elimina los espacios en blanco al principio y al final.\n",
    "    texto = str(texto).strip()\n",
    "    \n",
    "    # Eliminar tildes\n",
    "    texto = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', texto) # Descompone las letras con tildes. Por ejemplo, separa la 'á' en 'a' + '´'\n",
    "        if unicodedata.category(c) != 'Mn' # Elimina la parte del acento ('´') y deja solo la letra base\n",
    "    )\n",
    "    \n",
    "    # Convertir a mayúsculas\n",
    "    texto = texto.upper()\n",
    "    \n",
    "    # Eliminar caracteres especiales comunes\n",
    "    texto = texto.replace('D.C.', '').replace('DC', '').replace('.', '') # Quita puntos, \"D.C.\" y espacios extra\n",
    "    texto = texto.replace(',', '').replace('-', ' ').replace('_', ' ')\n",
    "    \n",
    "    # Normalizar espacios múltiples\n",
    "    texto = ' '.join(texto.split()) # Divide el texto en palabras y las vuelve a unir con un solo espacio. Esto elimina los dobles o triples espacios que quedan en medio de las palabras\n",
    "    \n",
    "    return texto\n",
    "\n",
    "\n",
    "def validar_input_departamento(nombre, df_localidades):\n",
    "    \"\"\"\n",
    "    Valida y limpia el input del departamento antes de procesarlo.\n",
    "    Retorna el nombre validado o lanza error con sugerencias.\n",
    "    \"\"\"\n",
    "    if not nombre or not isinstance(nombre, str): # evita que la función se rompa si encuentra NA\n",
    "        raise ValueError(\"El nombre del departamento no puede estar vacío\")\n",
    "    \n",
    "    # Limpiar input\n",
    "    nombre_limpio = limpiar_texto(nombre)\n",
    "    \n",
    "    if not nombre_limpio:\n",
    "        raise ValueError(\"El nombre del departamento no es válido después de la limpieza\")\n",
    "    \n",
    "    # Limpiar nombres del catálogo\n",
    "    df_localidades['DEPARTAMENTO_LIMPIO'] = df_localidades['DEPARTAMENTO'].apply(limpiar_texto)\n",
    "    departamentos_disponibles = df_localidades['DEPARTAMENTO_LIMPIO'].unique()\n",
    "    \n",
    "    # Buscar coincidencia exacta\n",
    "    if nombre_limpio in departamentos_disponibles:\n",
    "        # Retornar el nombre original del catálogo\n",
    "        return df_localidades[df_localidades['DEPARTAMENTO_LIMPIO'] == nombre_limpio]['DEPARTAMENTO'].iloc[0]\n",
    "    \n",
    "    # Buscar coincidencia parcial\n",
    "    similares = [d for d in departamentos_disponibles if nombre_limpio in d or d in nombre_limpio] # Si no hubo coincidencia exacta, busca si el texto del usuario está contenido dentro de algún nombre de la lista (búsqueda parcial)\n",
    "    \n",
    "    if similares: # \n",
    "        print(f\"'{nombre}' no encontrado exactamente\")\n",
    "        print(f\"¿Quisiste decir alguno de estos?\")\n",
    "        for sim in similares[:5]: # Si encuentra parecidos, imprime una lista de sugerencias\n",
    "            original = df_localidades[df_localidades['DEPARTAMENTO_LIMPIO'] == sim]['DEPARTAMENTO'].iloc[0]\n",
    "            print(f\"      - {original}\")\n",
    "    \n",
    "    raise ValueError(f\"Departamento no válido: '{nombre}'\") # detiene el código con un error\n",
    "\n",
    "\n",
    "def validar_input_municipio(nombre, departamento_validado, df_localidades):\n",
    "    \"\"\"\n",
    "    Valida y limpia el input del municipio antes de procesarlo.\n",
    "    Retorna el nombre validado o lanza error con sugerencias.\n",
    "    \"\"\"\n",
    "    if not nombre or not isinstance(nombre, str):\n",
    "        raise ValueError(\"El nombre del municipio no puede estar vacío\")\n",
    "    \n",
    "    # Limpiar input\n",
    "    nombre_limpio = limpiar_texto(nombre)\n",
    "    \n",
    "    if not nombre_limpio:\n",
    "        raise ValueError(\"El nombre del municipio no es válido después de la limpieza\")\n",
    "    \n",
    "    # Filtrar municipios del departamento\n",
    "    df_mun = df_localidades[df_localidades['DEPARTAMENTO'] == departamento_validado].copy()\n",
    "    df_mun['MUNICIPIO_LIMPIO'] = df_mun['MUNICIPIO'].apply(limpiar_texto)\n",
    "    municipios_disponibles = df_mun['MUNICIPIO_LIMPIO'].unique()\n",
    "    \n",
    "    # Buscar coincidencia exacta\n",
    "    if nombre_limpio in municipios_disponibles:\n",
    "        return df_mun[df_mun['MUNICIPIO_LIMPIO'] == nombre_limpio]['MUNICIPIO'].iloc[0]\n",
    "    \n",
    "    # Buscar coincidencia parcial\n",
    "    similares = [m for m in municipios_disponibles if nombre_limpio in m or m in nombre_limpio]\n",
    "    \n",
    "    if similares:\n",
    "        print(f\"'{nombre}' no encontrado exactamente en {departamento_validado}\")\n",
    "        print(f\"¿Quisiste decir alguno de estos?\")\n",
    "        for sim in similares[:5]:\n",
    "            original = df_mun[df_mun['MUNICIPIO_LIMPIO'] == sim]['MUNICIPIO'].iloc[0]\n",
    "            print(f\"      - {original}\")\n",
    "    \n",
    "    raise ValueError(f\"Municipio no válido: '{nombre}' en {departamento_validado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Interfaz de Usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de datos\n",
    "RUTA_SECTORES_URBANOS = \"abfss://dl-bronce@kodatasciencedatalake.dfs.core.windows.net/GEOESTADISTICO_COLOMBIA/DIVISION_POLITICA/SECTORES_URBANOS\"\n",
    "RUTA_DEPARTAMENTOS_POLITICO = \"abfss://dl-bronce@kodatasciencedatalake.dfs.core.windows.net/GEOESTADISTICO_COLOMBIA/DIVISION_POLITICA/DEPARTAMENTOS\"\n",
    "RUTA_LOCALIDADES = \"abfss://dl-bronce@kodatasciencedatalake.dfs.core.windows.net/GEOESTADISTICO_COLOMBIA/DIVISION_POLITICA/CATALOGO_LOCALIDADES\"\n",
    "\n",
    "# Cargar catálogo de localidades\n",
    "try:\n",
    "    # Leemos con Spark\n",
    "    df_localidades_spark = spark.read.parquet(RUTA_LOCALIDADES)\n",
    "    \n",
    "    # Convertimos a Pandas para poder usar lógica de validación e inputs interactivos\n",
    "    #    (Solo hacemos esto porque el catálogo es 'pequeño' y cabe en memoria)\n",
    "    df_localidades = df_localidades_spark.toPandas()\n",
    "    \n",
    "    # Normalización básica de columnas (opcional, asegura que sean strings)\n",
    "    df_localidades['DEPARTAMENTO'] = df_localidades['DEPARTAMENTO'].astype(str)\n",
    "    df_localidades['MUNICIPIO'] = df_localidades['MUNICIPIO'].astype(str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error crítico cargando catálogo desde Azure: {e}\")\n",
    "    spark.stop()\n",
    "    raise\n",
    "\n",
    "# Solicitar tipo de análisis\n",
    "tipo_analisis = input(\"\\n¿Desea analizar por DEPARTAMENTO completo o por MUNICIPIO específico? (departamento/municipio): \").strip().lower() # quita espacios extra y lo convierte a minúsculas el input\n",
    "\n",
    "# Manejo de errores\n",
    "if tipo_analisis not in [\"departamento\", \"municipio\"]:\n",
    "    print(\"Opción no válida. Se usará 'municipio' por defecto.\")\n",
    "    tipo_analisis = \"municipio\"\n",
    "\n",
    "# Solicitar departamento\n",
    "departamentos_disponibles = df_localidades['DEPARTAMENTO'].unique()\n",
    "print(f\"\\nDepartamentos disponibles: {len(departamentos_disponibles)}\")\n",
    "print(f\"Ejemplos: {departamentos_disponibles[:5].tolist()}\")\n",
    "\n",
    "departamento_input = input(\"\\nIngrese el nombre del DEPARTAMENTO: \").strip()\n",
    "\n",
    "# VALIDAR INPUT ANTES DE PROCESAR\n",
    "try:\n",
    "    departamento_nombre = validar_input_departamento(departamento_input, df_localidades) # llama a la función validar_input_departamento, le pasa lo que escribió el usuario (_input) y recibe el nombre oficial correcto (_nombre)\n",
    "    print(f\"Departamento validado: {departamento_nombre}\")\n",
    "except ValueError as e: # Si la función de validación no encontró nada (ni exacto ni parecido), captura el error\n",
    "    print(f\"\\nError de validación: {e}\")\n",
    "    raise # Detiene el programa\n",
    "\n",
    "# Variables\n",
    "municipio_nombre = None # Inicializa la variable vacía. Si el análisis es por departamento completo, esta variable se quedará así (None)\n",
    "analizar_departamento_completo = (tipo_analisis == \"departamento\")\n",
    "\n",
    "if tipo_analisis == \"municipio\":\n",
    "    # Filtrar municipios de ese departamento\n",
    "    municipios_dpto = df_localidades[df_localidades['DEPARTAMENTO'] == departamento_nombre]['MUNICIPIO'].unique()\n",
    "    \n",
    "    print(f\"\\nMunicipios disponibles en {departamento_nombre}: {len(municipios_dpto)}\")\n",
    "    print(f\"Ejemplos: {municipios_dpto[:10].tolist()}\")\n",
    "    \n",
    "    municipio_input = input(f\"\\nIngrese el nombre del MUNICIPIO: \").strip()\n",
    "    \n",
    "    # Validar\n",
    "    try:\n",
    "        municipio_nombre = validar_input_municipio(municipio_input, departamento_nombre, df_localidades)\n",
    "        print(f\"Municipio validado: {municipio_nombre}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError de validación: {e}\")\n",
    "        raise\n",
    "    \n",
    "    print(f\"\\nConfiguración: Análisis de municipio específico\")\n",
    "    print(f\"Departamento: {departamento_nombre}\")\n",
    "    print(f\"Municipio: {municipio_nombre}\")\n",
    "else:\n",
    "    print(f\"\\nConfiguración: Análisis de departamento completo\")\n",
    "    print(f\"Departamento: {departamento_nombre}\")\n",
    "    print(f\"Se procesarán TODOS los municipios del departamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Obtención del resto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAR GEOMETRÍA DEL DEPARTAMENTO\n",
    "try:\n",
    "    df_depto_geo = spark.read.parquet(RUTA_DEPARTAMENTOS_POLITICO)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar la geometría del departamento: {e}\")\n",
    "\n",
    "# CARGAR SECTORES URBANOS\n",
    "try:\n",
    "    df_sectores = spark.read.parquet(RUTA_SECTORES_URBANOS)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando sectores: {e}\")\n",
    "\n",
    "# CARGAR ATRACTORES (POIs)\n",
    "def obtener_ruta_pois_archivo(dpto_nombre, base_path):\n",
    "    nombre = dpto_nombre.lower().strip()\n",
    "    nombre = nombre.replace(\",\", \"\").replace(\" \", \"_\")\n",
    "    return f\"{base_path}/pois_{nombre}.csv\"\n",
    "\n",
    "RUTA_BASE_ATRACTORES = \"abfss://dl-bronce@kodatasciencedatalake.dfs.core.windows.net/GEOESTADISTICO_COLOMBIA/ATRACTORES\"\n",
    "ruta_archivo_pois = obtener_ruta_pois_archivo(departamento_nombre, RUTA_BASE_ATRACTORES)\n",
    "\n",
    "try:\n",
    "    df_pois = spark.read.parquet(ruta_archivo_pois)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudieron cargar POIs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## visualizacion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_localidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depto_geo_pd = df_depto_geo.toPandas()\n",
    "\n",
    "df_depto_geo_pd['geometry'] = gpd.GeoSeries.from_wkt(df_depto_geo_pd['geometry'])\n",
    "gdf_dpto_geo = gpd.GeoDataFrame(df_depto_geo_pd, geometry='geometry', crs=EPSG_WGS84)  # ← Agregar crs\n",
    "\n",
    "gdf_dpto_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectores_pd = df_sectores.toPandas()\n",
    "\n",
    "df_sectores_pd['geometry'] = gpd.GeoSeries.from_wkt(df_sectores_pd['geometry'])\n",
    "gdf_sectores = gpd.GeoDataFrame(df_sectores_pd, geometry='geometry', crs=EPSG_WGS84)  # ← Agregar crs\n",
    "\n",
    "gdf_sectores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pois_pd = df_pois.toPandas()\n",
    "df_pois_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Vinculación Geoespacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_codigo_departamento(departamento_nombre_validado, gdf_dpto_geo_input):\n",
    "    \"\"\"\n",
    "    Obtiene el código del departamento desde TODO_COLOMBIA_2024_DPTO_POLITICO.\n",
    "    NOTA: departamento_nombre_validado ya viene limpio y validado.\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuscando código para departamento: {departamento_nombre_validado}\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar shapefile de departamentos\n",
    "        gdf_dptos = gdf_dpto_geo_input.copy()\n",
    "        print(f\"   {len(gdf_dptos)} departamentos cargados\")\n",
    "        \n",
    "        # Limpiar nombres del shapefile\n",
    "        gdf_dptos['dpto_cnmbr_limpio'] = gdf_dptos['dpto_cnmbr'].apply(limpiar_texto)\n",
    "        nombre_buscar = limpiar_texto(departamento_nombre_validado)\n",
    "        \n",
    "        \n",
    "        # Buscar coincidencia\n",
    "        gdf_dpto = gdf_dptos[gdf_dptos['dpto_cnmbr_limpio'] == nombre_buscar].copy()\n",
    "        \n",
    "        # Si no encuentra, buscar parcial\n",
    "        if gdf_dpto.empty:\n",
    "            print(f\"Búsqueda exacta sin resultados, intentando búsqueda parcial...\")\n",
    "            gdf_dpto = gdf_dptos[\n",
    "                gdf_dptos['dpto_cnmbr_limpio'].str.contains(nombre_buscar, na=False) # Busca si el nombre está contenido dentro de alguno del archivo si escribiste \"Santander\", encontrará \"Norte de Santander\" y \"Santander\"\n",
    "            ].copy()\n",
    "        \n",
    "        # Si aún está vacío, mostrar opciones\n",
    "        if gdf_dpto.empty:\n",
    "            print(f\"\\nNo se encontró '{departamento_nombre_validado}' en el shapefile\")\n",
    "            print(f\"Departamentos disponibles en el shapefile:\")\n",
    "            for idx, row in gdf_dptos[['dpto_cnmbr', 'dpto_ccdgo']].head(10).iterrows(): # Ayuda al usuario. Imprime los primeros 10 nombres que SÍ existen en el archivo para que veas por qué falló la búsqueda\n",
    "                print(f\"      - {row['dpto_cnmbr']} (código: {row['dpto_ccdgo']})\")\n",
    "            raise ValueError(f\"No se encontró el departamento en el shapefile\")\n",
    "        \n",
    "        # Si hay múltiples, tomar el primero\n",
    "        if len(gdf_dpto) > 1: # Si la búsqueda trajo más de un resultado\n",
    "            print(f\"Se encontraron {len(gdf_dpto)} coincidencias, usando la primera\")\n",
    "        \n",
    "        # Obtener código\n",
    "        dpto_codigo = gdf_dpto.iloc[0]['dpto_ccdgo']\n",
    "        dpto_nombre_real = gdf_dpto.iloc[0]['dpto_cnmbr']\n",
    "        \n",
    "        print(f\"Departamento encontrado en shapefile:\")\n",
    "        print(f\"      Nombre: {dpto_nombre_real}\")\n",
    "        print(f\"      Código: {dpto_codigo}\")\n",
    "        \n",
    "        return dpto_codigo, gdf_dpto.to_crs(EPSG_WGS84) # El mapa de ese departamento convertido a coordenadas Latitud/Longitud (EPSG:4326)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Función de Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_sectores_por_departamento(dpto_codigo, gdf_sectores_input, \n",
    "                                     municipio_nombre_validado, df_localidades):\n",
    "    \"\"\"\n",
    "    Filtra sectores urbanos por departamento (y opcionalmente por municipio).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nFILTRANDO SECTORES URBANOS...\")\n",
    "    print(f\"Departamento código: {dpto_codigo}\")\n",
    "    if municipio_nombre_validado: # dar el municipio en caso de haber elegido por municipio\n",
    "        print(f\"   Municipio: {municipio_nombre_validado}\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar sectores urbanos\n",
    "        gdf_sectores = gdf_sectores_input.copy()\n",
    "        print(f\"   Total sectores Colombia: {len(gdf_sectores)}\")\n",
    "        \n",
    "        # Asegurar CRS\n",
    "        gdf_sectores = gdf_sectores.to_crs(EPSG_WGS84)\n",
    "        \n",
    "        # Filtrar por departamento usando 'dpto_ccdgo' (columna real)\n",
    "        if 'dpto_ccdgo' not in gdf_sectores.columns:\n",
    "            raise ValueError(\"Columna 'dpto_ccdgo' no encontrada en sectores urbanos\")\n",
    "        \n",
    "        gdf_filtrado = gdf_sectores[gdf_sectores['dpto_ccdgo'].astype(str) == str(dpto_codigo)].copy()\n",
    "        print(f\"{len(gdf_filtrado)} sectores del departamento\")\n",
    "        \n",
    "        if gdf_filtrado.empty:\n",
    "            raise ValueError(\"No se encontraron sectores para el departamento especificado\")\n",
    "        \n",
    "        # Vincular nombres de municipio desde RUTA_LOCALIDADES\n",
    "        # Preparar catálogo\n",
    "        df_loc = df_localidades.copy()\n",
    "        df_loc['CODIGO_MUNICIPIO'] = df_loc['CODIGO_MUNICIPIO'].astype(str).str.zfill(5) # Asegura que los códigos tengan 5 dígitos rellenando con ceros a la izquierda (ej. convierte \"5001\" en \"05001\")\n",
    "        \n",
    "        # Vincular usando mpio_ccdgo (columna real en sectores urbanos)\n",
    "        if 'mpio_ccdgo' in gdf_filtrado.columns: # verifica que exista la columna en los sectores filtrados\n",
    "            gdf_filtrado['mpio_ccdgo_str'] = gdf_filtrado['mpio_ccdgo'].astype(str).str.zfill(5) # hace lo mismo rellena con ceros para que coincida con el catálogo\n",
    "            \n",
    "            # Merge con catálogo\n",
    "            gdf_filtrado = gdf_filtrado.merge( # Pega la información del catálogo (df_loc) en el mapa (gdf_filtrado) usando el código como llave común.\n",
    "                df_loc[['CODIGO_MUNICIPIO', 'MUNICIPIO', 'DEPARTAMENTO']],\n",
    "                left_on='mpio_ccdgo_str',\n",
    "                right_on='CODIGO_MUNICIPIO',\n",
    "                how='left'  # how='left': Significa \"mantén todos los sectores del mapa, y si encuentras el nombre en el catálogo, pégalo; si no, déjalo vacío\".\n",
    "            )\n",
    "            \n",
    "            # Renombrar columna\n",
    "            gdf_filtrado = gdf_filtrado.rename(columns={'MUNICIPIO': 'nombre_municipio'}) # Cambia el nombre de la columna pegada\n",
    "            \n",
    "            print(f\"Nombres vinculados: {gdf_filtrado['nombre_municipio'].notna().sum()} sectores\")\n",
    "        else:\n",
    "            print(f\"Columna 'mpio_ccdgo' no encontrada, no se pueden vincular nombres\")\n",
    "            gdf_filtrado['nombre_municipio'] = 'DESCONOCIDO'\n",
    "        \n",
    "        # Filtrar por municipio si se especificó\n",
    "        if municipio_nombre_validado:\n",
    "            if 'nombre_municipio' in gdf_filtrado.columns:\n",
    "                gdf_filtrado = gdf_filtrado[\n",
    "                    gdf_filtrado['nombre_municipio'] == municipio_nombre_validado].copy()\n",
    "                print(f\"{len(gdf_filtrado)} sectores del municipio {municipio_nombre_validado}\")\n",
    "                \n",
    "                if gdf_filtrado.empty:\n",
    "                    raise ValueError(f\"No se encontraron sectores para el municipio {municipio_nombre_validado}\")\n",
    "        \n",
    "        # Generar microzona_id\n",
    "        # Formato: DPTO_MPIO_SECTOR (usando códigos que existen en todos los países)\n",
    "        print(f\"Generando microzona_id...\")\n",
    "        \n",
    "        componentes = []\n",
    "        if 'dpto_ccdgo' in gdf_filtrado.columns:\n",
    "            componentes.append(gdf_filtrado['dpto_ccdgo'].astype(str).str.zfill(2))\n",
    "        if 'mpio_ccdgo' in gdf_filtrado.columns:\n",
    "            componentes.append(gdf_filtrado['mpio_ccdgo'].astype(str).str.zfill(3))\n",
    "        if 'setu_ccdgo' in gdf_filtrado.columns:\n",
    "            componentes.append(gdf_filtrado['setu_ccdgo'].astype(str).str.zfill(4))\n",
    "        \n",
    "        if componentes:\n",
    "            gdf_filtrado['microzona_id'] = componentes[0]\n",
    "            for comp in componentes[1:]:\n",
    "                gdf_filtrado['microzona_id'] = gdf_filtrado['microzona_id'] + '_' + comp\n",
    "        else:\n",
    "            # Fallback: usar índice\n",
    "            gdf_filtrado['microzona_id'] = 'MZ_' + gdf_filtrado.index.astype(str).str.zfill(6) # Si faltan columnas, crea un ID de emergencia usando el número de fila (MZ_000123)\n",
    "        \n",
    "        print(f\"microzona_id generado para {len(gdf_filtrado)} sectores\")\n",
    "        \n",
    "        return gdf_filtrado\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Ingesta y Filtrado de Puntos de Interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_establecimientos_csv(df_pois_pd_input, gdf_sectores):\n",
    "    \"\"\"\n",
    "    Carga establecimientos desde CSV y filtra por el área de los sectores.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        # Cargar CSV\n",
    "        df = df_pois_pd_input\n",
    "        print(f\"{len(df)} establecimientos\")\n",
    "        \n",
    "        # Validar columnas de coordenadas\n",
    "        if 'coord_x' not in df.columns or 'coord_y' not in df.columns:\n",
    "            raise ValueError(\"Columnas 'coord_x' y 'coord_y' no encontradas en el CSV\")\n",
    "        \n",
    "        # Eliminar NaN\n",
    "        df = df.dropna(subset=['coord_x', 'coord_y'])\n",
    "        \n",
    "        # Crear GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['coord_x'], df['coord_y']), crs=EPSG_WGS84)\n",
    "        \n",
    "        print(f\"{len(gdf)} con coordenadas válidas\")\n",
    "        \n",
    "        # Filtrar por bbox de sectores\n",
    "        bbox = gdf_sectores.total_bounds # Calcula la \"Caja Delimitadora\" (Bounding Box) de tus sectores urbanos (Coordenada mínima X, mínima Y, máxima X, máxima Y)\n",
    "        gdf_filtrado = gdf.cx[bbox[0]:bbox[2], bbox[1]:bbox[3]].copy() # En lugar de verificar punto por punto si está dentro de un polígono complejo (que es lento), GeoPandas usa el índice espacial cx para decir: \"Dame todo lo que esté dentro de este rectángulo\"\n",
    "        \n",
    "        print(f\"{len(gdf_filtrado)} establecimientos en el área de interés\")\n",
    "        \n",
    "        return gdf_filtrado\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return gpd.GeoDataFrame(columns=['geometry'], crs=EPSG_WGS84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Descarga de Red Vial desde OpenStreetMap (OSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_vialidades(gdf_zona):\n",
    "    \"\"\"Descarga vialidades de OSM para el área.\"\"\"\n",
    "    print(\"\\nDESCARGANDO VIALIDADES  desde OSM...\")\n",
    "    \n",
    "    tags_vialidades = {\n",
    "        \"highway\": [\n",
    "            \"primary\", \"secondary\", \"tertiary\",\n",
    "            \"motorway\", \"trunk\",                 # solo las calles por donde circulan carros. Esto excluye automáticamente caminos peatonales (footway), ciclovías (cycleway) o caminos de tierra menores\n",
    "            \"primary_link\", \"secondary_link\",\n",
    "            \"tertiary_link\", \"motorway_link\", \"trunk_link\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        gdf_zona = gdf_zona.to_crs(EPSG_WGS84)\n",
    "        poly = gdf_zona.unary_union # gdf_zona puede tener 100 sectores (100 polígonos pequeños). Si OSM que busca en cada uno, harás 100 peticiones \n",
    "                                    # y será lentísimo unary_union fusiona todos esos en un solo polígono, se hace una sola petición al servidor \n",
    "        \n",
    "        if poly is None or poly.is_empty:\n",
    "            raise ValueError(\"Polígono inválido\")\n",
    "        \n",
    "        # Descargando desde OSM\n",
    "        gdf_vias = ox.features.features_from_polygon(poly, tags=tags_vialidades) # librería osmnx para conectarse a la API de OpenStreetMap\n",
    "                                                                                 # envía el polígono grande (poly) y la lista de tipos de calle (tags)\n",
    "        \n",
    "        if gdf_vias.empty:\n",
    "            print(\"No se encontraron vías\") # OSM no devolvió nada\n",
    "            return gpd.GeoDataFrame(columns=['geometry'], crs=EPSG_WGS84) # devuelve un mapa vacío para no romper el código\n",
    "        \n",
    "        gdf_vias = gdf_vias[\n",
    "            gdf_vias.geometry.type.isin(['LineString', 'MultiLineString']) # OSM marca un semáforo o una rotonda como un \"Punto\" o un \"Polígono\". Este filtro elimina todo lo que no sean Líneas\n",
    "        ].copy()\n",
    "        gdf_vias = gdf_vias.to_crs(EPSG_WGS84).reset_index(drop=True) # Reorganiza la tabla para que el índice sea limpio (0, 1, 2...)\n",
    "        \n",
    "        print(f\"{len(gdf_vias)} vialidades descargadas\")\n",
    "        return gdf_vias\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return gpd.GeoDataFrame(columns=['geometry'], crs=EPSG_WGS84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Generación de microzonas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Motor de procesamiento geométrico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_sectores_con_vialidades(gdf_sectores, gdf_vialidades, buffer_metros=8):\n",
    "    \"\"\"Divide sectores usando vialidades.\"\"\"\n",
    "    print(f\"\\nDIVIDIENDO SECTORES (buffer: {buffer_metros}m)...\")\n",
    "    \n",
    "    gdf_m = gdf_sectores.to_crs(EPSG_M) # Convierte el mapa de sectores a Metros (EPSG:3857) Las operaciones geométricas como cortar o medir \n",
    "                                        # distancias SIEMPRE deben hacerse en metros, no en grados (latitud/longitud), para ser precisas\n",
    "    \n",
    "    if gdf_vialidades.empty:\n",
    "        print(\"Sin vialidades, usando sectores completos\")\n",
    "        res = gdf_sectores.copy() # Copia los sectores tal cual\n",
    "        res[\"sub_microzona_id\"] = res[\"microzona_id\"] + \"_001\" # Les crea un ID nuevo terminando en _001 y retorna\n",
    "        return res\n",
    "        \n",
    "    vias_m = gdf_vialidades.to_crs(EPSG_M) # Convierte también las calles a Metros\n",
    "    microzonas = [] # Para guardar los pedasos recortados\n",
    "    \n",
    "    for _, row in tqdm(gdf_m.iterrows(), total=len(gdf_m), desc=\"   Cortando\"): # Inicia un bucle para procesar sector por sector con barra de progreso\n",
    "        geom = row.geometry                                                     # Extrae la forma del sector actual\n",
    "        vias_cercanas = vias_m[vias_m.intersects(geom.buffer(20))]              # En lugar de usar todas las calles de la ciudad para cortar un barrio, selecciona solo las calles que tocan o están a menos de 20 metros de ese barrio\n",
    "        \n",
    "        if vias_cercanas.empty:                                                 # Si no hay calles cerca, guarda el sector entero y pasa al siguiente\n",
    "            microzonas.append(row)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            cuchilla = vias_cercanas.buffer(buffer_metros).unary_union        # Convierte las líneas de las calles en polígonos gruesos (de 8 metros de ancho, según el parámetro) y los fusiona en una sola forma\n",
    "            recorte = geom.difference(cuchilla)                               # Resta la \"cuchilla\" (calles) al \"sector\" (barrio). Lo que queda son las manzanas aisladas\n",
    "            \n",
    "            if recorte.geom_type == \"Polygon\" and not recorte.is_empty:       # Si el resultado es un solo pedazo y no está vacío, lo guarda\n",
    "                row.geometry = recorte\n",
    "                microzonas.append(row)\n",
    "            elif recorte.geom_type == \"MultiPolygon\":                         # Si el corte generó varias islas (manzanas separadas)               \n",
    "                for p in recorte.geoms:                                       # Recorre cada isla          \n",
    "                    if p.area > 500:                                          # Si el pedazo es muy pequeño (menos de 500m²), lo descarta y si es grande, lo guarda como una nueva microzona\n",
    "                        new_row = row.copy()\n",
    "                        new_row.geometry = p\n",
    "                        microzonas.append(new_row)\n",
    "            else:\n",
    "                microzonas.append(row)\n",
    "        except:\n",
    "            microzonas.append(row)\n",
    "            \n",
    "    gdf_final = gpd.GeoDataFrame(microzonas, crs=EPSG_M).to_crs(EPSG_WGS84) # Convierte todos los pedacitos resultantes de nuevo a Latitud/Longitud\n",
    "    gdf_final = gdf_final.reset_index(drop=True)\n",
    "    \n",
    "    # Generar sub-ID\n",
    "    gdf_final[\"sub_id\"] = gdf_final.groupby(\"microzona_id\").cumcount() + 1               # Enumera los pedazos (1, 2, 3...) dentro de cada sector original\n",
    "    gdf_final[\"sub_microzona_id\"] = (\n",
    "        gdf_final[\"microzona_id\"] + \"_S\" + gdf_final[\"sub_id\"].astype(str).str.zfill(3)   # Crea el ID final único (ej. 05_001_SEC1_S001)\n",
    "    )\n",
    "    \n",
    "    print(f\"{len(gdf_final)} microzonas generadas\")\n",
    "    return gdf_final\n",
    "\n",
    "\n",
    "def contar_y_clasificar(gdf_micro, gdf_puntos):\n",
    "    \"\"\"Cuenta y clasifica.\"\"\"\n",
    "    print(\"Contando establecimientos...\")\n",
    "\n",
    "    # Asegura que ambos mapas (zonas y puntos) estén en el mismo sistema (Lat/Lon)\n",
    "    gdf_micro = gdf_micro.to_crs(EPSG_WGS84)\n",
    "    gdf_puntos = gdf_puntos.to_crs(EPSG_WGS84)\n",
    "    \n",
    "    # Usar sub_microzona_id si existe, sino microzona_id\n",
    "    id_col = 'sub_microzona_id' if 'sub_microzona_id' in gdf_micro.columns else 'microzona_id' # Decide qué columna usar como llave. Si ya cortamos con calles, usa sub_microzona_id; si no, usa microzona_id\n",
    "    \n",
    "    joined = gpd.sjoin(gdf_puntos, gdf_micro[[id_col, 'geometry']], predicate='within') # Asigna cada punto al polígono que lo contiene.\n",
    "    \n",
    "    if joined.empty:                                 # Si ningún punto cayó dentro de ninguna zona\n",
    "        gdf_micro['total_establecimientos'] = 0\n",
    "        gdf_micro['categoria'] = 'chica'\n",
    "        return gdf_micro\n",
    "        \n",
    "    conteo = joined.groupby(id_col).size()        # Cuenta cuántos puntos hay por cada ID de zona\n",
    "    gdf_micro = gdf_micro.set_index(id_col)\n",
    "    gdf_micro['total_establecimientos'] = conteo\n",
    "    gdf_micro['total_establecimientos'] = gdf_micro['total_establecimientos'].fillna(0).astype(int) # Las zonas que no tienen tiendas quedan con NaN (nulo); esto las convierte en 0\n",
    "    gdf_micro = gdf_micro.reset_index()\n",
    "    \n",
    "    def clasificar(n): # funcion uinterna, usa el diccionario RANGOS\n",
    "        for cat, (min_v, max_v) in RANGOS.items():   # Recorre los rangos (0-50, 50-100, etc.) y devuelve la etiqueta correspondiente.\n",
    "            if min_v <= n < max_v:\n",
    "                return cat\n",
    "        return 'separar'\n",
    "    \n",
    "    gdf_micro['categoria'] = gdf_micro['total_establecimientos'].apply(clasificar)\n",
    "    return gdf_micro\n",
    "\n",
    "\n",
    "def dividir_microzonas_grandes_voronoi(gdf):\n",
    "    \"\"\"Divide grandes con Voronoi.\"\"\"\n",
    "    print(\"\\nREFINANDO GRANDES...\")\n",
    "    \n",
    "    gdf = gdf.to_crs(EPSG_M) # Convertir para calcular áreas\n",
    "    grandes = gdf[gdf['categoria'].isin(['grande', 'separar'])].copy() # Separa las zonas que necesitan dividirse\n",
    "    otras = gdf[~gdf['categoria'].isin(['grande', 'separar'])].copy() # Separa las zonas que ya están bien\n",
    "    \n",
    "    if grandes.empty:\n",
    "        return gdf.to_crs(EPSG_WGS84)\n",
    "        \n",
    "    procesadas = []\n",
    "    id_col = 'sub_microzona_id' if 'sub_microzona_id' in grandes.columns else 'microzona_id'\n",
    "    \n",
    "    for _, row in tqdm(grandes.iterrows(), total=len(grandes), desc=\"   Voronoi\"): # Recorre las zonas grandes tqdm para barra de progreso\n",
    "        geom = row.geometry\n",
    "        total = row.total_establecimientos\n",
    "        n_partes = int(np.ceil(total / 75)) # Calcula en cuántos pedazos dividir\n",
    "        \n",
    "        if n_partes < 2:\n",
    "            procesadas.append(row)\n",
    "            continue\n",
    "            \n",
    "        puntos = [] # Genera puntos aleatorios dentro del polígono para usarlos como \"semillas\" del corte\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        intentos = 0\n",
    "        while len(puntos) < n_partes and intentos < n_partes*20:\n",
    "            p = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))\n",
    "            if geom.contains(p):\n",
    "                puntos.append(p)\n",
    "            intentos += 1\n",
    "            \n",
    "        if len(puntos) < 2:\n",
    "            procesadas.append(row)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            region = geom.buffer(100)\n",
    "            vor = voronoi_diagram(unary_union(puntos), envelope=region) # Algoritmo Voronoi. Crea regiones geométricas basándose en la cercanía a los puntos semilla\n",
    "            sub_id = 1\n",
    "            for poly in vor.geoms:\n",
    "                res = geom.intersection(poly) # Recorta el diagrama Voronoi infinito para que encaje exactamente dentro de la forma de la microzona original\n",
    "                if not res.is_empty and res.area > 50:\n",
    "                    new_row = row.copy()\n",
    "                    new_row.geometry = res\n",
    "                    new_row['total_establecimientos'] = int(total / n_partes) # Divide los puntos equitativamente entre los nuevos pedazos (es una estimación)\n",
    "                    new_row[id_col] = f\"{row[id_col]}_V{sub_id}\" # Agrega un sufijo al ID (ej. ..._S001_V1) para indicar que fue dividido por Voronoi\n",
    "                    procesadas.append(new_row)\n",
    "                    sub_id += 1\n",
    "        except:\n",
    "            procesadas.append(row)\n",
    "            \n",
    "    final = pd.concat([gpd.GeoDataFrame(procesadas, crs=EPSG_M), otras], ignore_index=True)\n",
    "    return final.to_crs(EPSG_WGS84)\n",
    "\n",
    "def unir_microzonas_chicas(gdf):\n",
    "    \"\"\"\n",
    "    Une microzonas chicas colindantes para formar microzonas medianas.\n",
    "    OPTIMIZADA: Usa índice espacial (sindex) para búsqueda rápida.\n",
    "    \"\"\"\n",
    "    print(\"\\nUNIENDO MICROZONAS CHICAS...\")\n",
    "    \n",
    "    # Trabajar en metros es obligatorio para geometría precisa\n",
    "    gdf = gdf.to_crs(EPSG_M)\n",
    "    \n",
    "    # Separar las chicas del resto\n",
    "    chicas = gdf[gdf['categoria'] == 'chica'].copy().reset_index(drop=True)\n",
    "    otras = gdf[gdf['categoria'] != 'chica'].copy()\n",
    "    \n",
    "    if chicas.empty or len(chicas) < 2:                            # Si no hay zonas chicas, devuelve el mapa original y termina\n",
    "        print(\"No hay suficientes microzonas chicas para unir\")\n",
    "        return gdf.to_crs(EPSG_WGS84)\n",
    "    \n",
    "    print(f\"   {len(chicas)} microzonas chicas para procesar\")\n",
    "    \n",
    "    # Variables de control\n",
    "    chicas['processed'] = False # olumna de control (tipo \"Checklist\") para saber qué zonas ya fusionamos y no repetirlas\n",
    "    id_col = 'sub_microzona_id' if 'sub_microzona_id' in chicas.columns else 'microzona_id'  # Define qué nombre de columna usar para los IDs\n",
    "    uniones = []\n",
    "    grupo_id = 1\n",
    "    \n",
    "    for idx, row in tqdm(chicas.iterrows(), total=len(chicas), desc=\"   Agrupando\"):   # Empieza a recorrer cada zona chica, una por una tqdm para barra de progreso\n",
    "        if chicas.at[idx, 'processed']: # Si esta zona ya fue fusionada en un paso anterior, la salta\n",
    "            continue\n",
    "        \n",
    "        # Iniciar nuevo grupo\n",
    "        grupo_geoms = [row.geometry]\n",
    "        grupo_ids = [str(row[id_col])]\n",
    "        grupo_total = row['total_establecimientos']\n",
    "        chicas.at[idx, 'processed'] = True # Marcar como usada\n",
    "\n",
    "        # Define las metas\n",
    "        target_min = 50\n",
    "        target_max = 100\n",
    "        \n",
    "        # Bucle de crecimiento del grupo\n",
    "        changed = True\n",
    "        while changed and grupo_total < target_max: # Mantiene el proceso vivo mientras siga encontrando vecinos válidos y no se haya llenado el grupo\n",
    "            changed = False\n",
    "            geom_actual = unary_union(grupo_geoms) # Forma actual del grupo\n",
    "            \n",
    "            # --- OPTIMIZACIÓN CON SINDEX ---\n",
    "            # 1. Filtro Rápido: Buscar solo geometrías cuyo recuadro (bbox) toque el grupo\n",
    "            posibles_indices = list(chicas.sindex.query(geom_actual, predicate='intersects'))\n",
    "            \n",
    "            # 2. Filtro Fino: Revisar uno por uno los candidatos\n",
    "            for candidato_idx in posibles_indices:\n",
    "                if candidato_idx == idx or chicas.at[candidato_idx, 'processed']: # Verifica si realmente están pegadas pared con pared\n",
    "                    continue\n",
    "                \n",
    "                row2 = chicas.loc[candidato_idx]\n",
    "                \n",
    "                # Verificar contacto real (touches = se tocan, intersects = se solapan)\n",
    "                if geom_actual.touches(row2.geometry) or geom_actual.intersects(row2.geometry):\n",
    "                    \n",
    "                    nuevo_total = grupo_total + row2['total_establecimientos'] # Suma cuántos establecimientos tendríamos si las unimos\n",
    "                    \n",
    "                    # Decidir si unimos\n",
    "                    if nuevo_total <= target_max or grupo_total < target_min: # Solo une si La suma no se pasa de 100 O si el grupo actual es tan pequeño (<50)\n",
    "                        grupo_geoms.append(row2.geometry) # Si pasa la prueba, agrega la vecina al equipo\n",
    "                        grupo_ids.append(str(row2[id_col]))\n",
    "                        grupo_total += row2['total_establecimientos']\n",
    "                        chicas.at[candidato_idx, 'processed'] = True # Marcar vecina como usada\n",
    "                        changed = True\n",
    "                        \n",
    "                        if grupo_total >= target_min:\n",
    "                            break # Ya cumplimos la meta, salir a cerrar el grupo\n",
    "        \n",
    "        # Guardar el resultado del grupo\n",
    "        nueva_geom = unary_union(grupo_geoms) # Funde todas las geometrías del equipo en un solo polígono final\n",
    "        nueva_row = row.copy() # Copiar datos base\n",
    "        nueva_row.geometry = nueva_geom\n",
    "        nueva_row['total_establecimientos'] = grupo_total\n",
    "        # Generar ID compuesto (ej. UNION_1_ID1-ID2)\n",
    "        ids_cortos = [i.split('_')[-1] for i in grupo_ids[:3]] # Solo tomar la última parte del ID para no hacerlo gigante\n",
    "        nueva_row[id_col] = f\"U{grupo_id}_{'-'.join(ids_cortos)}\" \n",
    "        \n",
    "        # Recalcular categoría\n",
    "        if grupo_total < 50: nueva_row['categoria'] = 'chica'\n",
    "        elif grupo_total < 100: nueva_row['categoria'] = 'mediana'\n",
    "        elif grupo_total < 200: nueva_row['categoria'] = 'grande'\n",
    "        else: nueva_row['categoria'] = 'separar'\n",
    "        \n",
    "        uniones.append(nueva_row)\n",
    "        grupo_id += 1\n",
    "    \n",
    "    # Unir resultados\n",
    "    if uniones:\n",
    "        gdf_uniones = gpd.GeoDataFrame(uniones, crs=EPSG_M)\n",
    "        # Traer de vuelta las que quedaron \"solteras\" (chicas que no se pudieron unir a nadie)\n",
    "        sobras = chicas[~chicas['processed']].copy()\n",
    "        \n",
    "        final = pd.concat([gdf_uniones, sobras, otras], ignore_index=True) # Pega todo de nuevo: (Las uniones nuevas) + (Las sobras chicas) + (Las medianas/grandes originales)\n",
    "        \n",
    "        print(f\"{len(uniones)} grupos nuevos creados\")\n",
    "        print(f\"   Distribución final:\")\n",
    "        print(final['categoria'].value_counts().to_string())\n",
    "        \n",
    "        return final.to_crs(EPSG_WGS84)\n",
    "    else:\n",
    "        return gdf.to_crs(EPSG_WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_colombia():\n",
    "    \"\"\"Pipeline completo para Colombia.\"\"\"\n",
    "    \n",
    "    # PASO 1: Obtener código de departamento\n",
    "    dpto_codigo, gdf_dpto = obtener_codigo_departamento(\n",
    "        departamento_nombre,\n",
    "        gdf_dpto_geo\n",
    "    )\n",
    "    \n",
    "    # PASO 2: Filtrar sectores (con vinculación de nombres)\n",
    "    print(\"\\nPASO 2: Filtrando sectores...\")\n",
    "    gdf_sectores_filtrado = filtrar_sectores_por_departamento(\n",
    "        dpto_codigo,\n",
    "        gdf_sectores,\n",
    "        municipio_nombre,\n",
    "        df_localidades\n",
    "    )\n",
    "    \n",
    "    # PASO 3: Cargar establecimientos\n",
    "    print(\"\\nPASO 3: Cargando establecimientos...\")\n",
    "    gdf_establecimientos = cargar_establecimientos_csv(df_pois_pd, gdf_sectores_filtrado)\n",
    "\n",
    "    # --- DEFINICIÓN DEL ESTADO INICIAL (Sectores sin dividir) ---\n",
    "    print(\"\\nPASO 3.5: Generando Dataset INICIAL (Sectores sin dividir)...\")\n",
    "    microzonas_inicial = contar_y_clasificar(gdf_sectores_filtrado.copy(), gdf_establecimientos)\n",
    "    \n",
    "    # PASO 4: Obtener vialidades\n",
    "    print(\"\\nPASO 4: Obteniendo vialidades...\")\n",
    "    vialidades = obtener_vialidades(gdf_sectores_filtrado)\n",
    "    \n",
    "    # PASO 5: Dividir sectores\n",
    "    print(\"\\nPASO 5: Dividiendo sectores...\")\n",
    "    microzonas_div = dividir_sectores_con_vialidades(gdf_sectores_filtrado, vialidades)\n",
    "    \n",
    "    # PASO 6: Contar (Conteo intermedio de las zonas divididas)\n",
    "    print(\"\\nPASO 6: Contando establecimientos...\")\n",
    "    microzonas_intermedio = contar_y_clasificar(microzonas_div, gdf_establecimientos)\n",
    "    \n",
    "    # PASO 7: Refinar grandes\n",
    "    print(\"\\nPASO 7: Refinando grandes...\")\n",
    "    microzonas_refinadas = dividir_microzonas_grandes_voronoi(microzonas_intermedio.copy())\n",
    "    \n",
    "    # PASO 8: Reclasificar después de dividir grandes\n",
    "    print(\"\\nPASO 8: Reclasificación...\")\n",
    "    microzonas_reclasificadas = contar_y_clasificar(microzonas_refinadas, gdf_establecimientos)\n",
    "    \n",
    "    # PASO 9: Unir microzonas chicas colindantes\n",
    "    print(\"\\nPASO 9: Uniendo microzonas chicas...\")\n",
    "    microzonas_unidas = unir_microzonas_chicas(microzonas_reclasificadas.copy())\n",
    "    \n",
    "    # PASO 10: Reclasificar final después de unir\n",
    "    print(\"\\n PASO 10: Reclasificación final...\")\n",
    "    microzonas_final = contar_y_clasificar(microzonas_unidas, gdf_establecimientos)\n",
    "    \n",
    "    # PASO 11: Eliminar vacías\n",
    "    print(\"\\nPASO 11: Eliminando vacías...\")\n",
    "    antes = len(microzonas_final)\n",
    "    microzonas_final = microzonas_final[microzonas_final['total_establecimientos'] > 0].copy()\n",
    "    print(f\"   Eliminadas: {antes - len(microzonas_final)}\")\n",
    "    \n",
    "    # PASO 12: Calcular área\n",
    "    print(\"\\nPASO 12: Calculando áreas...\")\n",
    "    gdf_temp = microzonas_final.to_crs(EPSG_M)\n",
    "    microzonas_final['area_km2'] = (gdf_temp.geometry.area / 1_000_000).round(4)\n",
    "\n",
    "    # PASO 13: LIMPIEZA DE COLUMNAS\n",
    "    print(\"\\nPASO 13: Generando Datasets Limpios (Inicial y Final)...\")\n",
    "    \n",
    "    #  A. FUNCION DE REPARACIÓN INTERNA\n",
    "    def reparar_dataset(gdf_sucio, tipo=\"final\"):\n",
    "        df = gdf_sucio.copy()\n",
    "        \n",
    "        # 1. Definir el ID Correcto\n",
    "        id_col = 'sub_microzona_id' if 'sub_microzona_id' in df.columns else 'microzona_id'\n",
    "        \n",
    "        # 2. Reconstruir códigos desde el ID (El ID nunca se pierde)\n",
    "        # Formato ID típico: \"05_001_XXXX\" (Dpto_Mpio_...)\n",
    "        try:\n",
    "            # Dpto es la parte 0, Mpio es la parte 1\n",
    "            df['temp_dpto_cod'] = df[id_col].astype(str).apply(lambda x: x.split('_')[0])\n",
    "            df['temp_mpio_cod'] = df[id_col].astype(str).apply(lambda x: x.split('_')[1])\n",
    "            \n",
    "            # Construimos el código DANE completo (5 dígitos) para buscar el nombre\n",
    "            df['temp_dane_completo'] = df['temp_dpto_cod'] + df['temp_mpio_cod']\n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia reparando códigos en dataset {tipo}: {e}\")\n",
    "            # Fallback si el ID no tiene el formato esperado\n",
    "            df['temp_dpto_cod'] = str(dpto_codigo).zfill(2)\n",
    "            df['temp_dane_completo'] = \"00000\"\n",
    "\n",
    "        # 3. Preparar Diccionario de Nombres (Desde df_localidades que es nuestra fuente de verdad)\n",
    "        df_loc = df_localidades.copy()\n",
    "        df_loc['CODIGO_MUNICIPIO'] = df_loc['CODIGO_MUNICIPIO'].astype(str).str.zfill(5)\n",
    "        mapa_nombres = df_loc.set_index('CODIGO_MUNICIPIO')['MUNICIPIO'].to_dict()\n",
    "        mapa_dptos = df_loc.set_index('CODIGO_MUNICIPIO')['DEPARTAMENTO'].to_dict()\n",
    "        \n",
    "        # 4. Asignar Valores (Reparación)\n",
    "        df['codigo_departamento'] = df['temp_dpto_cod']\n",
    "        df['codigo_municipio'] = df['temp_dane_completo']\n",
    "        \n",
    "        # Mapear nombres (si no encuentra el código, pone 'DESCONOCIDO')\n",
    "        df['nombre_municipio'] = df['temp_dane_completo'].map(mapa_nombres).fillna('DESCONOCIDO')\n",
    "        df['nombre_departamento'] = df['temp_dane_completo'].map(mapa_dptos).fillna('DESCONOCIDO')\n",
    "        \n",
    "        # Si el departamento salió desconocido (raro), forzamos el nombre global\n",
    "        mask_unknown = df['nombre_departamento'] == 'DESCONOCIDO'\n",
    "        if mask_unknown.any():\n",
    "            # Intentamos obtener el nombre del departamento input\n",
    "            try:\n",
    "                nombre_dpto_global = df_localidades[df_localidades['DEPARTAMENTO_LIMPIO'].str.contains(limpiar_texto(str(departamento_nombre)), na=False)]['DEPARTAMENTO'].iloc[0]\n",
    "            except:\n",
    "                nombre_dpto_global = \"DESCONOCIDO\"\n",
    "            df.loc[mask_unknown, 'nombre_departamento'] = nombre_dpto_global\n",
    "\n",
    "        # 5. Calcular Área si falta (para el inicial)\n",
    "        if 'area_km2' not in df.columns:\n",
    "            df['area_km2'] = (df.to_crs(EPSG_M).geometry.area / 1_000_000).round(4)\n",
    "            \n",
    "        # 6. Selección Final de Columnas\n",
    "        columnas_finales = {\n",
    "            id_col: 'id_microzona',\n",
    "            'geometry': 'geometry',\n",
    "            'area_km2': 'area_km2',\n",
    "            'nombre_departamento': 'nombre_departamento',\n",
    "            'codigo_departamento': 'codigo_departamento',\n",
    "            'nombre_municipio': 'nombre_municipio',\n",
    "            'codigo_municipio': 'codigo_municipio',\n",
    "            'categoria': 'categoria',\n",
    "            'total_establecimientos': 'total_establecimientos'\n",
    "        }\n",
    "        \n",
    "        # Filtrar y Renombrar\n",
    "        cols_validas = {k: v for k, v in columnas_finales.items() if k in df.columns}\n",
    "        df_limpio = df[list(cols_validas.keys())].rename(columns=cols_validas)\n",
    "        \n",
    "        return df_limpio.to_crs(EPSG_WGS84)\n",
    "\n",
    "    # B. APLICAR REPARACIÓN\n",
    "    print(\"Reparando dataset FINAL...\")\n",
    "    dataset_final_limpio = reparar_dataset(microzonas_final, tipo=\"final\")\n",
    "    \n",
    "    print(\"Reparando dataset INICIAL...\")\n",
    "    dataset_inicial_limpio = reparar_dataset(microzonas_inicial, tipo=\"inicial\")\n",
    "\n",
    "    # Reporte final\n",
    "    print(\"\\nREPORTE FINAL DE MICROZONAS\")\n",
    "    print(\"\\nDistribución por categoría:\")\n",
    "    print(microzonas_final['categoria'].value_counts().to_string())\n",
    "\n",
    "    return dataset_inicial_limpio, dataset_final_limpio, vialidades, gdf_dpto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "microzonas_inicial, microzonas_final, vialidades, poligono_dpto = pipeline_colombia()\n",
    "\n",
    "print(f\"\\n RESULTADOS:\")\n",
    "print(f\"   Iniciales: {len(microzonas_inicial)}\")\n",
    "print(f\"   Finales: {len(microzonas_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "microzonas_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "microzonas_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_mapa_microzonas_bogota(microzonas, vialidades, establecimientos_totales, titulo=\"Microzonas\"):\n",
    "    \"\"\"\n",
    "    Genera mapa interactivo.\n",
    "    - Muestra microzonas con tooltip completo.\n",
    "    - Muestra SOLO los establecimientos que caen dentro de los polígonos mostrados.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerando mapa: {titulo}\")\n",
    "    \n",
    "    # Crear mapa base\n",
    "    centro = microzonas.to_crs(EPSG_M).geometry.centroid.to_crs(EPSG_WGS84)\n",
    "    m = folium.Map(\n",
    "        location=[centro.y.mean(), centro.x.mean()], \n",
    "        zoom_start=13, \n",
    "        tiles=\"Cartodb Positron\",\n",
    "        prefer_canvas=True\n",
    "    )\n",
    "    \n",
    "    # 1. Agregar Vialidades (Fondo)\n",
    "    if not vialidades.empty:\n",
    "        folium.GeoJson(\n",
    "            vialidades, name=\"Vialidades\", \n",
    "            style_function=lambda x: {\"color\": \"#ff7f00\", \"weight\": 1.5, \"opacity\": 0.5}\n",
    "        ).add_to(m)\n",
    "\n",
    "    # 2. Configurar Tooltips y Colores\n",
    "    paleta = {\"chica\": \"#92c5de\", \"mediana\": \"#f4a582\", \"grande\": \"#ca0020\", \"separar\": \"#7b3294\"}\n",
    "    \n",
    "    # Estas son las columnas NUEVAS que definiste en el pipeline\n",
    "    campos_tooltip = ['id_microzona', 'area_km2', 'nombre_municipio', 'nombre_departamento', 'categoria', 'total_establecimientos']\n",
    "    # Validar que existan (por seguridad)\n",
    "    campos_reales = [c for c in campos_tooltip if c in microzonas.columns]\n",
    "    \n",
    "    # 3. Dibujar Zonas\n",
    "    for cat, color in paleta.items():\n",
    "        capa = microzonas[microzonas[\"categoria\"] == cat].copy()\n",
    "        if not capa.empty:\n",
    "            fg = folium.FeatureGroup(name=f\"Zonas: {cat.upper()}\", show=True)\n",
    "            \n",
    "            folium.GeoJson(\n",
    "                capa,\n",
    "                style_function=lambda feature, color=color: {\n",
    "                    \"fillColor\": color, \"color\": \"black\", \"weight\": 0.5, \"fillOpacity\": 0.5\n",
    "                },\n",
    "                highlight_function=lambda feature: {\"fillOpacity\": 0.8, \"weight\": 2, \"color\": \"white\"},\n",
    "                tooltip=folium.GeoJsonTooltip(\n",
    "                    fields=campos_reales,\n",
    "                    aliases=[c.replace('_', ' ').title() + ':' for c in campos_reales],\n",
    "                    style=\"font-family: Arial; font-size: 12px;\"\n",
    "                )\n",
    "            ).add_to(fg)\n",
    "            fg.add_to(m)\n",
    "\n",
    "    # 4. Dibujar Establecimientos (Filtrados por polígono)\n",
    "    print(\"Filtrando y dibujando establecimientos...\")\n",
    "    \n",
    "    # A. Filtro espacial rápido (Bounding Box)\n",
    "    minx, miny, maxx, maxy = microzonas.total_bounds\n",
    "    puntos_cerca = establecimientos_totales.cx[minx:maxx, miny:maxy]\n",
    "    \n",
    "    # B. Filtro fino: Solo los que están DENTRO de las geometrías de este dataset\n",
    "    # Usamos sjoin para quedarnos solo con los puntos que caen en estas microzonas específicas\n",
    "    puntos_filtrados = gpd.sjoin(puntos_cerca, microzonas[['geometry']], predicate='within')\n",
    "    \n",
    "    if not puntos_filtrados.empty:\n",
    "        fg_puntos = folium.FeatureGroup(name=\"Establecimientos\", show=False)\n",
    "        \n",
    "        # Validar columnas del CSV de establecimientos\n",
    "        col_name = 'name' if 'name' in puntos_filtrados.columns else puntos_filtrados.columns[0]\n",
    "        col_amenity = 'amenity' if 'amenity' in puntos_filtrados.columns else 'N/A'\n",
    "        \n",
    "        folium.GeoJson(\n",
    "            puntos_filtrados,\n",
    "            marker=folium.CircleMarker(radius=2, fill=True, fill_opacity=1),\n",
    "            style_function=lambda x: {\"color\": \"#333333\", \"fillColor\": \"#333333\", \"weight\": 0},\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=[col_name, col_amenity],\n",
    "                aliases=['Nombre:', 'Tipo:'],\n",
    "                style=\"font-family: Arial; font-size: 11px; font-weight: bold;\"\n",
    "            )\n",
    "        ).add_to(fg_puntos)\n",
    "        fg_puntos.add_to(m)\n",
    "\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recargar establecimientos (necesario para pintar los puntos)\n",
    "print(\"\\nRecargando puntos para el mapa...\")\n",
    "pts_totales = cargar_establecimientos_csv(df_pois_pd, poligono_dpto)\n",
    "\n",
    "# Mapa Inicial\n",
    "display(mostrar_mapa_microzonas_bogota(\n",
    "    microzonas_inicial, vialidades, pts_totales, titulo=\"INICIAL\"\n",
    "))\n",
    "\n",
    "# 4. Mapa Final\n",
    "display(mostrar_mapa_microzonas_bogota(\n",
    "    microzonas_final, vialidades, pts_totales, titulo=\"FINAL\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
